{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreated on Mon Jun 10 15:40:22 2019\\n\\n@author: athomas7\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Jun 10 15:40:22 2019\n",
    "\n",
    "@author: athomas7\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_file = \"C:/SEPP19/Eye Tracking/Single Eye/train + test.zip\"\\n\\nimport zipfile\\n\\nzf = zipfile.ZipFile(train_file)\\nzf.extractall()\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "train_file = \"C:/SEPP19/Eye Tracking/Single Eye/train + test.zip\"\n",
    "\n",
    "import zipfile\n",
    "\n",
    "zf = zipfile.ZipFile(train_file)\n",
    "zf.extractall()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tempfile\n",
    "\n",
    "def load_image(file_path):\n",
    "    return cv2.imread(file_path)\n",
    "\n",
    "def extract_label(file_name):\n",
    "    return 1 if \"open\" in file_name else 0 # open eyes are 1 & closed eyes are 0\n",
    "\n",
    "train_path = \"C:/Users/Z/Desktop/begintrain/\"\n",
    "\n",
    "image_files = os.listdir(train_path)\n",
    "train_images = [load_image(train_path + file) for file in image_files]\n",
    "train_labels = [extract_label(file) for file in image_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will probably not be an issue with the real deal, since all images will be the same size\n",
    "# so we can re-train with that view from the pilot's eye from the instrument...\n",
    "def preprocess_image(img, side = 28): # number of pixels on the smallest side\n",
    "    # average eye aspect ratio is 1.87 by 1 (it requires an int, so I rounded 1.87 to 2)\n",
    "    eye_aspect_ratio = 1\n",
    "    min_side = min(img.shape[0], img.shape[1])\n",
    "    img = img[:min_side, :min_side * eye_aspect_ratio]\n",
    "    img = cv2.resize(img, (side * eye_aspect_ratio, side)) # average eye aspect ratio of 1.87 by 1\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return img / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15589d42a88>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "preview_index = 186\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(train_images[preview_index])\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(preprocess_image(train_images[preview_index]), cmap=\"gray\")\n",
    "# some images are showing up wonky here b/c of your aspect ratio side multiplier ^^^\n",
    "# it does allow to get the entire eye within the frame though so worth it...\n",
    "# looks weird to us but the neural net will understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_images)):\n",
    "    train_images[i] = preprocess_image(train_images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_images = np.expand_dims(train_images, axis = -1)\n",
    "train_labels = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(187, 28, 28, 1) (187,)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow: 2.2.0-dev20200304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' TensorFlow 2.0.0 Beta\\nmodel.compile(optimizer=tf.optimizers.Adam(),\\n              loss=tf.losses.SparseCategoricalCrossentropy(),\\n              metrics=[tf.metrics.SparseCategoricalAccuracy()])\\n'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow:\", tf.__version__)\n",
    "\n",
    "layers = [\n",
    "    tf.keras.layers.Conv2D(filters=16, kernel_size=(3,3), padding=\"same\", activation=tf.nn.relu, input_shape=train_images.shape[1:]),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding=\"same\", activation=tf.nn.relu, input_shape=train_images.shape[1:]),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=tf.nn.relu, input_shape=train_images.shape[1:]),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=tf.nn.relu, input_shape=train_images.shape[1:]),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n",
    "#     tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=tf.nn.relu, input_shape=train_images.shape[1:]),\n",
    "#     tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=512, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(units=256, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(units=2, activation=tf.nn.softmax) # probability for each of the classes (2 as of now)\n",
    "]\n",
    "\n",
    "# https://keras.io/optimizers/ (see Adam section)\n",
    "# https://keras.io/losses/ (see sparse_categorical_accuracy)\n",
    "model = tf.keras.Sequential(layers)\n",
    "model.compile(optimizer='adam',\n",
    "              # optimizer=tf.keras.optimizers.Adam(),\n",
    "              # loss='binary_crossentropy',\n",
    "              # loss=tf.losses.sparse_softmax_cross_entropy(),\n",
    "              # loss=tf.keras.backend.sparse_categorical_crossentropy(),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              # metrics=[tf.metrics.accuracy()])\n",
    "\"\"\" TensorFlow 2.0.0 Beta\n",
    "model.compile(optimizer=tf.optimizers.Adam(),\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[tf.metrics.SparseCategoricalAccuracy()])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.6919 - accuracy: 0.5080\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.6801 - accuracy: 0.6684\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.6471 - accuracy: 0.6257\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.6112 - accuracy: 0.6791\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.5464 - accuracy: 0.6952\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5027 - accuracy: 0.7754\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.3814 - accuracy: 0.8610\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.3372 - accuracy: 0.8663\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.2757 - accuracy: 0.8984\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.2470 - accuracy: 0.9091\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "model.fit(train_images, train_labels, epochs=10, batch_size=50)\n",
    "# model.save_weights(\"model.tf\")\n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-147-f0c64e5700a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \"\"\"\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0meval_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpreprocess_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muploads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# must be an array because of the for-loop below\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;31m# eval_images = [preprocess_image(load_image(file)) for file in uploads].keys()]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0meval_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-133-05ec88e8b6ca>\u001b[0m in \u001b[0;36mpreprocess_image\u001b[1;34m(img, side)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# average eye aspect ratio is 1.87 by 1 (it requires an int, so I rounded 1.87 to 2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0meye_aspect_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mmin_side\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmin_side\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mmin_side\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0meye_aspect_ratio\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mside\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0meye_aspect_ratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mside\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# average eye aspect ratio of 1.87 by 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "\n",
    "test_file = \"closed_eye_1.jpg\"\n",
    "uploads = cv2.imread(\"../datasets/eyes/train_blink_data/\" + test_file)\n",
    "\n",
    "\"\"\" For Multiple Images...\n",
    "import glob\n",
    "import cv2\n",
    "images = [cv2.imread(file) for file in glob.glob(\"path/to/files/*.png\")]\n",
    "\"\"\"\n",
    "\n",
    "eval_images = [preprocess_image(uploads)] # must be an array because of the for-loop below\n",
    "# eval_images = [preprocess_image(load_image(file)) for file in uploads].keys()]\n",
    "eval_model = tf.keras.Sequential(layers)\n",
    "eval_model.load_weights(\"model.h5\")\n",
    "eval_predictions = eval_model.predict(np.expand_dims(eval_images, axis = -1))\n",
    "\n",
    "cols = 4\n",
    "rows = np.ceil(len(eval_images)/cols)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(cols*4, rows*4)\n",
    "for i in range(len(eval_images)):\n",
    "    plt.subplot(rows, cols, i+1)\n",
    "    plt.imshow(eval_images[i], cmap=\"gray\")\n",
    "    plt.title(\"Open\" if np.argmax(eval_predictions[i])==1 else \"Closed\")\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-149-178caa3eccf1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimg_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muploads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-133-05ec88e8b6ca>\u001b[0m in \u001b[0;36mpreprocess_image\u001b[1;34m(img, side)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# average eye aspect ratio is 1.87 by 1 (it requires an int, so I rounded 1.87 to 2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0meye_aspect_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mmin_side\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmin_side\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mmin_side\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0meye_aspect_ratio\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mside\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0meye_aspect_ratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mside\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# average eye aspect ratio of 1.87 by 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "img_arr = np.array(preprocess_image(uploads))\n",
    "print(len(img_arr.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to:  C:\\Users\\Z\\AppData\\Local\\Temp\\tmpppfnogf4.h5\n"
     ]
    }
   ],
   "source": [
    "# Backend agnostic way to save/restore models\n",
    "_, keras_file = tempfile.mkstemp('.h5')\n",
    "print('Saving model to: ', keras_file)\n",
    "tf.keras.models.save_model(model, keras_file, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
